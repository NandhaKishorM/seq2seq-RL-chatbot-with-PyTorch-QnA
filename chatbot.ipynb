{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCXy-5bbFaHc",
        "colab_type": "text"
      },
      "source": [
        "# Import drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgloA0uAFZGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwusL4KoFmE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtIRyD_bFqgj",
        "colab_type": "text"
      },
      "source": [
        "# clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iurGRS9JUojr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6a31c061-4645-450e-fde8-242cbe4ea6d7"
      },
      "source": [
        "!git clone https://github.com/kishorkuttan/seq2seq-RL-chatbot-with-PyTorch-QnA"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seq2seq-RL-chatbot-with-PyTorch-QnA'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 48 (delta 2), reused 4 (delta 0), pack-reused 37\u001b[K\n",
            "Unpacking objects: 100% (48/48), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jtY1i0kqEKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "131e5087-6ea7-4ded-fcb0-51e41b037bcd"
      },
      "source": [
        "%cd \"/content/seq2seq-RL-chatbot-with-PyTorch-QnA\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seq2seq-RL-chatbot-with-PyTorch-QnA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH3XbT2xVFxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "60187847-8d4e-4f8e-c567-b469ab75aecc"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=15HOJmRizBrgoPPVDHKpvSO2tNf0k-d8f&export=download"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15HOJmRizBrgoPPVDHKpvSO2tNf0k-d8f\n",
            "To: /content/seq2seq-RL-chatbot-with-PyTorch-QnA/model.zip\n",
            "406MB [00:04, 86.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuI45q0BVIHz",
        "colab_type": "code",
        "outputId": "d6bc568c-c044-41c9-d3ae-778436df46ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!unzip model.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  model.zip\n",
            "  inflating: model/vocab.txt         \n",
            "  inflating: model/pytorch_model.bin  \n",
            "  inflating: model/config.json       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rxjnk1etVpK",
        "colab_type": "text"
      },
      "source": [
        "### Rename model/config.json to model/bert_config.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc_Hdvh7EZ8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv model/config.json  model/bert_config.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHKm18NEVWMX",
        "colab_type": "code",
        "outputId": "283ab835-5f74-4f1c-ac96-ef3a8356a7f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "!pip install pytorch_transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.13.19)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.18.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 26.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 33.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 21.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 16.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 11.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 11.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 11.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 10.7MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 10.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.5.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.16.19)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.15.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch_transformers) (0.16.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch_transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch_transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=49eb126420fdabae3d87081209f2fdfebd70b95ceb60e3c3df11ac6447273c5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.43 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0911DcC3JZk",
        "colab_type": "code",
        "outputId": "b754f460-8b69-4897-afb4-bc8996184616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 3.7MB/s \n",
            "\u001b[?25hCollecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 38.8MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.5.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.13.19)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 101kB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/79/e2a8d4809827f29ff0c67fc1e7d6ad8a678844e67717787c54cb34aec087/gevent-20.6.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 8.5MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 56.8MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/43/2402fd1f63992a52f88e3b169d59674617013bf7f1ad0cd7d842eafd0c58/numpydoc-1.0.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/01/0c/e4da4191474e27bc41bedab2bf249b27d9261db749f59769d7e7ca8feead/responses-0.10.14-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 58.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->allennlp) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (47.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.16.19)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.2.2)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.6MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 62.0MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.16; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/a4/0d8685c98986326534b0753a8b92b3082bc9df42b348bc50d6c69839c9f9/greenlet-0.4.16-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/96/361edb421a077a4c208b4a5c212737d78ae03ce67fbbcd01621c49f332d1/zope.event-4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.15.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.91)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->allennlp) (0.15.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Building wheels for collected packages: overrides, word2number, ftfy, jsonnet, parsimonious\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=41e4cad3f16a4de88affc22fca15860e3929c562380fa34f86e9a68ba5d01715\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=e8355fbf330f04c4daa141528dbc56354a0faf7e16fc69496949ab58fcfce6b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=72af8abebcaa3ac803ef0b80574447a32aaf787b07bf69736cb0799ea9f357b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321592 sha256=0ed0e780412750ced231eff5758fa6e3239e1b095c453b340b2a07d5a3dd0d98\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42712 sha256=2b54e6326134184696f46c966b037728b9a1e0ef537936e25b8c0c4510523c0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "Successfully built overrides word2number ftfy jsonnet parsimonious\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: conllu, tensorboardX, overrides, word2number, ftfy, preshed, blis, plac, thinc, spacy, zope.interface, greenlet, zope.event, gevent, jsonpickle, jsonnet, pytorch-pretrained-bert, flaky, flask-cors, numpydoc, responses, unidecode, parsimonious, pytorch-transformers, allennlp\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: pytorch-transformers 1.2.0\n",
            "    Uninstalling pytorch-transformers-1.2.0:\n",
            "      Successfully uninstalled pytorch-transformers-1.2.0\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.7 gevent-20.6.0 greenlet-0.4.16 jsonnet-0.16.0 jsonpickle-1.4.1 numpydoc-1.0.0 overrides-3.0.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.14 spacy-2.1.9 tensorboardX-2.0 thinc-7.0.8 unidecode-1.1.1 word2number-1.1 zope.event-4.4 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezgq16-666WE",
        "colab_type": "text"
      },
      "source": [
        "# PDF TO TEXT\n",
        "## upload a pdf file and we will use the text to get our questions answered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYT4FkdyEWHC",
        "colab_type": "code",
        "outputId": "16e996a3-e568-46a1-a0a7-80097b121cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "!pip install pdfplumber\n",
        "!pip install PyPDF2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/44/b1fc0e005094147c40a7751c8d94026971247c1f84348e9e37c9200bb6d6/pdfplumber-0.5.21.tar.gz\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from pdfplumber) (3.0.4)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/16/da16a22d47bac9bf9db39f3b9af74e8eeed8855c0df96be20b580ef92fff/pycryptodome-3.9.7-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 239kB/s \n",
            "\u001b[?25hCollecting unicodecsv>=0.14.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
            "Collecting pdfminer.six==20200104\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/bb/d38a297068723409193296f056c63fcefe6a8a784049b34bbdbf559b5118/pdfminer.six-20200104-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pdfplumber) (7.0.0)\n",
            "Collecting wand\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/f7/1d0907b2bb70a2d87795b60a8a195d5ba7aed88baf79c94534bb0d4b103e/Wand-0.6.1-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20200104->pdfplumber) (2.1.0)\n",
            "Building wheels for collected packages: pdfplumber, unicodecsv\n",
            "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfplumber: filename=pdfplumber-0.5.21-cp36-none-any.whl size=30947 sha256=7ce0743d556d3cfea5105d7cf9bc6ff12efe42dba47036eb922a5e70c4991ea8\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/54/10/a1ea81749d001048386ad90ad4d3fa2f3ea0225711faba1adf\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp36-none-any.whl size=10768 sha256=bae7faacc7cc2f181bffdb8db7516cb6d7dce1f6897dbe17945d46902dfe1cb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
            "Successfully built pdfplumber unicodecsv\n",
            "Installing collected packages: pycryptodome, unicodecsv, pdfminer.six, wand, pdfplumber\n",
            "Successfully installed pdfminer.six-20200104 pdfplumber-0.5.21 pycryptodome-3.9.7 unicodecsv-0.14.1 wand-0.6.1\n",
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp36-none-any.whl size=61086 sha256=6a534711e4fa054f9a17f92513df22915cc160d44aef3a612a7cdc85b8083b67\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj-EsORwFx1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "efc54fe6-f920-40f8-b973-c36aed6e33f4"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "api.py                                 model.zip\n",
            "bert.py                                \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            "bidaf-model-2017.09.15-charpad.tar.gz  README.md\n",
            "chatbot.ipynb                          report.txt\n",
            "\u001b[01;34mcorpus\u001b[0m/                                run.py\n",
            "data_utils.py                          \u001b[01;34msentiment_analysis\u001b[0m/\n",
            "\u001b[01;34mmodel\u001b[0m/                                 seq2seq_model.py\n",
            "model_conv_lstm_final.h5               seq2seq.py\n",
            "model_conv_lstm_final.json             tokenizer.pickle\n",
            "\u001b[01;34mmodel_RL\u001b[0m/                              utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpWf7GmA5YLA",
        "colab_type": "code",
        "outputId": "a5e9909d-90cf-4599-c09c-ca83b941f467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import pdfplumber\n",
        "import PyPDF2\n",
        "pdfFileObj = open('20-255695.pdf', 'rb')\n",
        "pdf = pdfplumber.open('20-255695.pdf')\n",
        "\n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "\n",
        "print(pdfReader.numPages)\n",
        "text_all = []\n",
        "for i in range(pdfReader.numPages):\n",
        "\n",
        "\n",
        "  page = pdf.pages[i]\n",
        "  text = page.extract_text()\n",
        "  text_all.append(text)\n",
        "  \n",
        " \n",
        "print(text_all )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "[' \\nCoronaTracker: World-wide COVID-19 \\nOutbreak Data Analysis and Prediction \\nCoronaTracker Community Research Group \\nFairoza Amira Binti Hamzaha, Cher Han Laub, Hafeez Nazric, Dominic Vincent \\nLigotd, Guanhua Leee, Cheng Liang Tanf, Mohammad Khursani Bin Mohd Shaibg, \\nUmmi Hasanah Binti Zaidonh, Adina Binti Abdullahi, Ming Hong Chungj, Chin Hwee \\nOngk, Pei Ying Chewl and Roland Emmanuel Salungam  \\n \\na The Kyoto College of Graduate Studies for Informatics \\nb Lead \\nc Media Prima Digital Sdn Bhd \\nd Cirrolytix \\ne National University Health System \\nf International Medical University \\ng Vase.ai \\nh Institut Wanita Berdaya Selangor \\ni University of Malaya \\nj Quest International University Perak \\nk ST Engineering \\nl Universiti Malaysia Sarawak \\nm National Capital Region, Philippines  \\nCorrespondence to Fairoza Amira Binti Hamzah (email: fairozaamira@gmail.com) \\n \\n(Submitted: 18 March 2020 – Published online: 19 March 2020) \\n \\nDISCLAIMER \\nThis paper was submitted to the Bulletin of the World Health Organization and was posted to \\nthe COVID-19 open site, according to the protocol for public health emergencies for \\ninternational concern as described in Vasee Moorthy et al. \\n(http://dx.doi.org/10.2471/BLT.20.251561).  \\n \\nThe information herein is available for unrestricted use, distribution and reproduction in any \\nmedium, provided that the original work is properly cited as indicated by the Creative \\nCommons Attribution 3.0 Intergovernmental Organizations licence (CC BY IGO 3.0).  \\n \\nRECOMMENDED CITATION \\nBinti Hamzah FA, Lau C, Nazri H, Ligot DV, Lee G, Tan CL, et al. CoronaTracker: World-\\nwide COVID-19 Outbreak Data Analysis and Prediction. [Preprint]. Bull World Health Organ. \\nE-pub: 19 March 2020. doi: http://dx.doi.org/10.2471/BLT.20.255695 \\n  1 ', 'CoronaTracker: World-wide COVID-19 Outbreak Data \\nAnalysis and Prediction \\nCoronaTracker Community Research Group \\nAbstract \\nBackground \\nCOVID-19 outbreak was first reported in Wuhan, China and has spread to more than 50 countries. \\nWHO declared COVID-19 as a Public Health Emergency of International Concern (PHEIC) on 30 \\nJanuary 2020. Naturally, a rising infectious disease involves fast spreading, endangering the health \\nof large numbers of people, and thus requires immediate actions to prevent the disease at the \\ncommunity level. Therefore, CoronaTracker was born as the online platform that provides latest \\nand reliable news development, as well as statistics and analysis on COVID-19. This paper is done \\nby the research team in the CoronaTracker community and aims to predict and forecast COVID-\\n19 cases, deaths, and recoveries through predictive modelling. The model helps to interpret patterns \\nof public sentiment on disseminating related health information, and assess political and economic \\ninfluence of the spread of the virus. \\nMethods: \\nReal-time data query is done and visualized in our website, then the queried data is used for \\nSusceptible-Exposed-Infectious-Recovered  (SEIR)  predictive  modelling.  We  utilize  SEIR \\nmodelling to forecast COVID-19 outbreak within and outside of China based on daily observations. \\nWe also analyze the queried news, and classify the news into negative and positive sentiments, to \\nunderstand the influence of the news to people’s behavior both politically and economically.  \\nFindings: \\nAt the time of writing this paper, the number of confirmed cases is expected to exceed 76000 cases, ', 'and reach the peak of this outbreak before 20 February 2020. The average Infected-Suspected ratio \\nwas found to be 2.399 which we used to initialize the number of Exposed individuals as a product \\nof the number of Infected individuals on 20 Jan 2020. This outbreak is assumed to reach its peak \\nin late May 2020 and will start to drop around early July 2020. Based on the news queried in our \\nsystem, we found that there are more negative articles than positive articles, and displayed similar \\nwords  for  both  negative  and  positive  sentiments.  The  top  five  positive  articles  are  about \\ncollaboration and strength of individuals in facing this epidemic, and the top five negative articles \\nare related to uncertainty and poor outcome of the disease such as deaths. \\nConclusions: \\nCOVID-19 is still an unclear infectious disease, which means we can only obtain an accurate SEIR \\nprediction after the outbreak ends. The outbreak spreads are largely influenced by each country’s \\npolicy and social responsibility. As data transparency is crucial inside the government, it is also \\nour responsibility not to spread unverified news and to remain calm in this situation. The \\nCoronaTracker project has shown the importance of information dissemination that can help in \\nimproving response time, and help planning in advance to help reduce risk. Further studies need to \\nbe done to help contain the outbreak as soon as possible.  \\nKeywords: COVID-19, data analysis, sentiment analysis, predictive modelling, SEIR \\n \\n1.  Introduction \\nOn 31 December 2019, the first reported case in the COVID-19 outbreak was reported in \\nWuhan, China. The first case outside of China was reported in Thailand on 13 January 2020 [1]. \\nSince then, this ongoing outbreak has now spread to more than 50 other countries [2].  WHO \\ndeclares COVID-19 outbreak as a Public Health Emergency of International Concern (PHEIC) by \\nWHO on 30 January 2020 [3]. There are over 76,000 cases of confirmed COVID-19 worldwide as ', 'of 20 February [2].  \\nAn infectious disease outbreak is the occurrence of a disease that is not usually expected \\nin a particular community, geographical region, or time period [4]. Typically, a rising infectious \\ndisease involves fast spreading, endangering the health of large numbers of people, and thus \\nrequires immediate action to prevent the disease at the community level [5]. COVID-19 is caused \\nby a new type of coronavirus which was previously named 2019-nCoV by the World Health \\nOrganization (WHO).  It is the seventh member of the coronavirus family, together with MERS-\\nnCoV and SARS-nCoV, that can spread to humans [1]. The symptoms of the infection include \\nfever, cough, shortness of breath, and diarrhea. In more severe cases, COVID-19 can cause \\npneumonia and even death [6]. The incubation period of COVID-19 can last for 2 weeks or longer \\n[7]. During the period of latent infection, the disease may still be infectious. The virus can spread \\nfrom person to person through respiratory droplets and close contact [8].  \\nAn ‘infodemic’ has accompanied the COVID-19 outbreak which is essentially an over-\\nabundance of information regarding the outbreak. As some of the information available to the \\npublic may not be accurate, it becomes hard for people to find reliable sources and trustworthy \\nguidance when they need it [9]. Because of the high demand for appropriate and trustworthy \\ninformation about 2019-nCoV, WHO technical risk communication and social media teams have \\nbeen working closely to track and respond to myths and rumors via its headquarters in Geneva, its \\nsix regional offices and its partners. The organization is working continuously to identify the most \\nwidespread rumors that can possibly harm the public’s health, such as inaccurate prevention \\nmeasures or claims of cures. These myths are then rebutted with evidence-based information. WHO \\nis making public health information and advice on the COVID-19, including myth busters, \\naccessible on its social media channels (including Weibo, Twitter, Facebook, Instagram, LinkedIn, \\nPinterest) and on their website [10].  ', 'Communication  during  emerging  pandemics  presents  a  distinctive  public  health \\neducation task. Health consumers must be informed about an impending health threat [11]. \\nHowever, there may be difficulties in providing accurate information regarding the outbreak in the \\ninitial stage. This is mainly related to the high degree of uncertainty about the exact route of \\ntransmission, treatment of the infections, and prospects of recovery in an outbreak.  All countries \\nneed  to  prepare  existing  public  health  communication  networks,  media  and  community \\nengagement staff for a possible case in their country, as well as for the appropriate response if it \\nhappens. The governments should coordinate communications with other response organizations \\nand include the community in response operations. WHO stands ready to coordinate with partners \\nto support countries in their communication and response to community engagement. \\nTo ensure a people-centered response to COVID-19, an expanding group of global \\nresponse organizations such as the United Nations Children’s Fund (UNICEF) and the International \\nFederation of Red Cross and Red Crescent Societies (IFRC) are coordinating efforts with WHO to  \\napply biomedical recommendations at the community level. These organizations are active at the \\nglobal, regional and country level to ensure that affected populations have a voice and are part of \\nthe response. Ensuring that global recommendations and communication are tested and adapted to \\nlocal contexts will help countries to gain better control over the COVID-19 outbreak [10]. \\nPeoples’ response to the news about a spreading contagious disease is likely to lead to \\nincreased anxiety and amplification of risk perceptions [11].   Social media networks have \\nfunctioned as channels for firsthand information from which the public can acquire disease-related \\ninformation during infectious disease outbreaks. These platforms also enable simple and quick \\nsharing of information with family, friends, and neighbors in real time [12]. For example, the \\nMinistry of Health in Malaysia have been uploading posts related to COVID-19 to educate the \\npublic since 19 January [13] and their Director General of Health is also active on his own ', 'Facebook page to clear confusion and doubts for the public [14]. This is important when traditional \\nforms of media are unable to provide relevant and timely information to the public. Social media \\nnow serves as a major, immediate information source but while the focus of latest information has \\nbeen on the role of social media during infectious disease outbreaks, the question that should be \\nbrought to light is still, how the use of social media may trigger the public’s emotional or \\nnoncognitive response, affect perception of risk, and preventive behaviors [10]. \\nTherefore, CoronaTracker [15] was born as the online platform that provides the latest \\nand  verified  news  development,  statistics  and  analysis  on  COVID-19. This  platform  is  a \\ncommunity-based project initiated on 25th February out of concerns on the outbreak that halted \\nMainland Chinese of Lunar New Year’s celebration. The CoronaTracker website was launched on \\n27th January 2020, after two days working relentlessly, and has gathered more than 1300 volunteers \\nacross the globe. This paper is a part of a work by the research team of CoronaTracker community. \\nThe main objective of this paper is to predict and forecast COVID-19 cases, deaths, and recoveries \\nthrough predictive modelling, and to decipher patterns on public sentiment related to health \\ninformation dissemination. At the same time, assess the political and economic impact of the virus \\nspread.  \\nWe propose a comprehensive framework to manage health information data as a tool for \\npublic health practitioners in managing epidemics and crafting public health response and policy. \\nThis study focuses on the role of audiences in the process of disseminating health risk information \\nand examines behaviors that contribute to information amplification upon hearing the news.  \\nThe  structure  of  this  paper  is  as  follows;  Section  1  introduces  COVID-19  and \\nCoronaTracker community, as well as explains the significance of this research. Section 2 describes \\non related works in predictive modelling of the paper and news-based sentiment analysis for this \\nresearch on psychological, politics and economics aspects. Section 3 explains our study design and ', 'methodologies. Section 4 presents our findings in current trends, predictive modelling and \\nsentiment analysis of the outbreak. Our findings are discussed in Section 5 and this paper is \\nconcluded in Section 6. \\n2.  Related Works \\nSEIR refers to Susceptible, Exposed, Infectious, and Removed or Recovered, respectively. \\nIt is based on the SIR model but adds the Exposed compartment as a variable. Susceptible refers \\nto individuals who can catch the infection and may become hosts if exposed, Exposed are \\nindividuals who are already infected but are asymptomatic, Infectious are individuals who are \\nshowing signs of infection and can transmit the virus, Removed or Recovered are individuals who \\nare previously infected but are no longer infectious and already immune to the virus [16].  \\nOnce the compartments of SIR or SEIR models are determined, modelling can be done \\nusing a variety of methods. In [17], a Conditional Autoregressive (CAR) was used to account for \\nepidemics with a spatial or transportation-related vector and modelled with MCMC. In [16], \\ndemographic effects such as birth and death rates were added to the SEIR to model equilibria with \\nvital dynamics.  \\nSentiment analysis is a supervised machine learning problem. There are different types \\nof sentiment analysis including fine-grained sentiment analysis, emotion detection, aspect-based \\nsentiment analysis and multilingual sentiment analysis. In binary sentiment classification, the \\npossible categories are positive and negative. In fine-grained sentiment classification, there are five \\ngroups (very negative, negative, neutral, positive, and very positive). Sentiment analysis is one of \\nthe most popular tasks in natural language processing, and there has been a lot of research and \\nprogress in solving this task accurately [18]. \\nDeep neural networks are widely used in sentiment polarity classification; however, it \\noften requires huge numbers of training data, and the size of training data varies quite significantly ', 'among domains. In [19], it was found that a dual-module approach is the best method that \\nencourages the learning of models with promising generalization abilities. Bidirectional Encoder \\nRepresentations  from Transformers  (BERT)  is  an  embedding  layer  designed  to  train  deep \\nbidirectional representations from unlabeled texts by jointly conditioning on both left and right \\ncontext in all layers. It is pretrained from a large unsupervised text corpus such as Wikipedia or \\nBookCorpus. There are 15% of the words in the input sequence are masked out which is one of the \\nobjectives of BERT. Then, a deep bidirectional Transformer encoder is fed by the entire sequences \\nso that the model learns to predict the masked words.  \\nMoreover, this small model has been trained on SST-2 dataset which is a common dataset \\nfor sentiment-analysis [20]. However, there are few disadvantages in this method as it is based on \\nSST-2 dataset which is for movie reviews and our dataset is about coronavirus news. It is a similar \\ntask which is for sentiment analysis but it does not perform that well because sentiment for movies \\nand news might be different. However, it is the fastest way to get results and act as a benchmark or \\nstarter for further research. It can also be easily improved by adding more dataset for our domain \\n(coronavirus news). Last but not least, it can do prediction instantly compared to previous methods \\nthat need bigger computer resources. \\n3. Methods \\n3.1 Data Source \\n  Data is extracted from verified sources such as John Hopkins University [21], WHO and \\nDingXiangYuan, a website authorized by the Chinese government. The sites reported confirmed \\nCOVID-19 cases, as well as recovered and deaths for affected countries and regions. Details on \\nhow our team fetched the data is in Section 3.2.  \\n3.2 Data Visualization \\n  The data collected in CoronaTracker is available on data lakes platform. Both the ', 'platform and dashboard are hosted in Amazon Web Services (AWS). We provisioned AWS \\nRelational Database Service (RDS) to host the data in MySQL table form. All the data collected \\nand ingested using Python program running in AWS Elastic Compute Cloud (EC2) and was \\nscheduled to automatically update every 15 minutes. The size of the database is relatively around \\n30GB.  \\n \\nFig. 1 High level diagram ingestion for CoronaTracker \\nWe developed our own micro-services and scraper in Python to fetch the data and news \\nfrom the sources. Python has been the best open source platform to use because of the less level of \\ncomplexity, readily libraries and easy to deploy in production environment (EC2). The code snippet \\nfor our data fetcher is shown as in Fig. 2. \\n \\nFig. 2 Code snippet for data fetcher ', 'The fetched data will be stored in relational database, MySQL. The micro-services will \\ncrawl the data every 15 minutes and store it in table form. The data mostly will be stored in raw \\nformat before being aggregated for visualization. Aggregate functions are built using SQL \\nstatement like below snippet. Aggregation is important to show latest of the sum values for every \\ncountry in the table. \\n \\nFig. 3 SQL statement and AGG (aggregate) functions \\nExample of the data in the databases is shown in Fig. 4. We also geo-coded the location \\nfor easier mapping during visualization. ', ' \\nFig. 4 Result from AGG query \\nFor every 15 min, the cumulative case counts will be updated for all provinces and other \\naffected countries. In the beginning, we found that WHO and JHU data are relatively slow \\ncompared to other sources, thus we implement manual update to our system after verifications to \\nallow more real-time data. \\n3.3 Predictive Modelling – SEIR Model \\nHere we will briefly discuss the properties of basic Susceptible-Exposed-Infected-\\nRemoved (SEIR) system that will be used to describe the recent outbreak of COVID-19 in China \\n[22]. We considered a simple SEIR epidemic model for the simulation of the infectious-disease \\nspread. Individuals were each assigned to one of the following disease states; Susceptible (S), \\nExposed (E), Infections (I) and Removed (R) which refers to segment not yet infected and disease-\\nfree,  individuals  that  are  experiencing  incubation  duration,  the  confirmed  (isolated)  cases, \\nrecovered individuals, respectively. The SEIR diagram in Fig. 5 shows how individuals move ', 'through each compartment in the model. \\n \\nFig. 5 SEIR model with four states [23] \\nParameters within this model are: \\n1.  β, controls the rate of spread, which represents the probability of transmitting disease between \\na susceptible and an infectious individual [24]. The reproductive number used in this paper is \\n2.2.  \\n2.  Incubation rate σ, is the rate of latent individuals becoming infectious. Given the known \\naverage duration of incubation Y, σ = 1/Y [24]. The average incubation duration of 5.2 days \\nare used here. \\n3.  Recovery rate γ = 1/D, is determined by the average duration of recovery D, of infection. After \\nthis period, they enter the removed phase. The average duration of infection is calculated as \\nthe average serial interval minus the average incubation duration. The average serial interval \\nof 7.5 days is used in this paper [24]. 2.3 days of an average infectious duration is used here. \\nFig. 6 shows the diagrammatic representation of virus progress in an individual, where \\ninfectious occurs at 𝑡 , during the latent period, infected individual is not infectious, and at 𝑡 , \\n(cid:3013) (cid:3046)(cid:3052)\\nsymptoms appear [16]. The first transmission to the left healthy individuals is at 𝑡 . After 𝑡 , the \\n(cid:3047)(cid:3045) (cid:3019)\\nremoved (recovered) people are considered no longer infectious. \\n \\nFig. 6 Virus progress in an individual by using the SEIR model ', 'We can describe the virus transmission by the following nonlinear ordinary differential \\nequation [23] as shown in equation (10) to (13). \\n(cid:3031)(cid:3020)= −(cid:3081)(cid:3020)(cid:3010),     (10) \\n(cid:3031)(cid:3047) (cid:3015)\\n(cid:3031)(cid:3006) =(cid:3081)(cid:3020)(cid:3010)−𝜎𝐸,   (11) \\n(cid:3031)(cid:3047) (cid:3015)\\n(cid:3031)(cid:3010) =𝜎𝐸−𝛾𝐼,    (12) \\n(cid:3031)(cid:3047)\\n(cid:3031)(cid:3019)=𝛾𝐼,         (13) \\n(cid:3031)(cid:3047)\\n3.4 Sentiment Analysis \\nAfter we have stored the news inside the CoronaTracker database, we extract news description \\nas it contains a summary of the news that is neither too short nor too long, which can be bad for \\nthe model we are going to use otherwise. We only select descriptions that are at least more than 8 \\nwords, and discard non-English descriptions because the pre-trained model we use have been \\ntrained on SST-2 [25], which is a dataset for sentiment analysis for English language. We use a \\nlibrary called transformers by huggingface [26]. The input sentences will be separated by their \\nrespective polarity for further analysis like topic modelling and generating word cloud for each \\npolarity. \\n4. Findings \\n4.1 Current Outbreak Trends \\nFig. 7 to Fig. 9 shows the current trends for COVID-19 outbreak as displayed in CoronaTracker \\nwebsite [27]. The cases reported are visualized in analytics dashboard to show the outbreak trend \\nfor confirmed, recovered and deaths cases for all regions and countries. This aligns with our \\nobjectives to show the outbreak progress over the period of time for each segment. It was found \\nthat the total number of confirmed cases for all countries and regions are increasing steadily, but \\non day 24, the huge increment with 15000 differences were discovered. This is due to the change ', 'in how China measured the confirmed cases. Fig. 9 is plotted by using an open source mapping \\nlibrary from Leaflet [28]. Table 1 shows the details on reported cases according to the countries \\n[27]. Both are retrieved on 3rd March 2020. \\n \\nFig. 7 Outbreak trend over time \\n \\n \\nFig. 8 Most affected regions ', ' \\nFig. 9 World map of affected region, where the darker the redness in the map, the more the \\nnumber of infected cases detected. \\nTable 1 Reported cases as of 3rd March 2020 \\nCountry  Total Confirmed  Total Recovered  Total Deaths \\n China  80,151  47,367  2,945 \\n South Korea  5,186  30  28 \\n Italy  2,036  149  52 \\n Iran  1,501  291  66 \\n Others*  706  10  6 \\n Japan  274  32  6 \\n France  191  12  3 \\n Germany  165  16  0 \\n Spain  120  2  0 \\n Singapore  108  78  0 \\n United States  105  7  6 ', ' Hong Kong  100  37  2 \\n Kuwait  56  0  0 \\n Bahrain  49  0  0 \\n Thailand  43  31  1 \\n Taiwan  42  12  1 \\n Switzerland  42  0  0 \\n United Kingdom  40  8  0 \\n Malaysia  36  22  0 \\n Australia  31  11  1 \\n Canada  27  6  0 \\n Iraq  26  0  0 \\n Norway  25  0  0 \\n United Arab Emirates  21  5  0 \\n Netherlands  18  0  0 \\n Austria  18  0  0 \\n Vietnam  16  16  0 \\n Sweden  15  0  0 \\n Lebanon  13  0  0 \\n Israel  12  1  0 \\n Macau  10  9  0 \\n Iceland  9  0  0 \\n Croatia  9  0  0 \\n Belgium  8  1  0 \\n San Marino  8  0  0 ', ' Greece  7  0  0 \\n Qatar  7  0  0 \\n Finland  6  1  0 \\n Oman  6  1  0 \\n Ecuador  6  0  0 \\n India  5  3  0 \\n Mexico  5  1  0 \\n Algeria  5  0  0 \\n Pakistan  5  0  0 \\n Denmark  4  0  0 \\n Czech Republic  4  0  0 \\n Russia  3  2  0 \\n Philippines  3  2  1 \\n Georgia  3  0  0 \\n Romania  3  0  0 \\n Azerbaijan  3  0  0 \\n Egypt  2  1  0 \\n Brazil  2  0  0 \\n Portugal  2  0  0 \\n Indonesia  2  0  0 \\n Cambodia  1  1  0 \\n Nepal  1  1  0 \\n Sri Lanka  1  1  0 \\n Afghanistan  1  0  0 ', ' Estonia  1  0  0 \\n Andorra  1  0  0 \\n Nigeria  1  0  0 \\n Luxembourg  1  0  0 \\n Saudi Arabia  1  0  0 \\n* Cases identified on a cruise ship currently in Japanese territorial waters. \\n4.2 Predictive Modelling \\nIn this section, we will model the global trajectory of the infection counts using the SEIR \\nmodel, 240 days from the start date of 20 January 2020.  This start date was chosen because earlier \\ndates were assumed as “burn-in” for the reporting of infection counts. The parameters of the SEIR \\nmodel were determined in section 3.2 and the world’s population are assumed to be 7.5 billion \\npeople. \\nUsing the data source from JHU, we lacked an initial number of Exposed individuals. To \\ngather this information, we made the estimation that the number of Infected individuals today is an \\napproximate ratio to the number of suspected individuals 6 days ago with an incubation duration \\nof 5.2 days; we rounded up the incubation duration from 5.2 days for this calculation. The data to \\ncalculate this ratio was collected via real-time query from DingXiangYuan which provided \\nsuspected and infected counts in mainland China. Using two weeks of data from 20 Jan 2020, the \\naverage Infected-Suspected ratio was found to be 2.399 which we used to initialize the number of \\nExposed individuals as a product of the number of Infected individuals on 20 Jan 2020. \\nUsing Scipy’s implementation [29] of the numerical integration of ordinary differential \\nequations, odeint, we plotted the E and I trajectories of the SEIR compartment. The Exposed and \\nInfected trajectories tell us the number of individuals in those compartments over time. It can be \\nseen from the plot that the maximum number of Infected individuals is 425.066 million globally ', 'on 23 May 2020. Thereafter, the number of Infected individuals dropped to under 10 million on 12 \\nJuly 2020, under 1 million on 3 Aug 2020 and under 10,000 at the end of the trajectory on 14 Sep \\n2020. \\n \\nFig. 10 Simulated Exposed and Infected Per Day \\nAlthough the SEIR model is a numerical simulation, the numbers provide us a degree to \\nwhich the COVID-19 cases can surge to. These trajectories could serve as a means for governments, \\nbusinesses and individuals to plan and mitigate for such a spike in Infected cases. Everyone should \\nwork towards blunting the curve and stopping the spread as per instructions set out by their \\ngovernments on personal hygiene, control measures and refraining from mass gatherings. \\n4.3 Sentiment Analysis \\nThis paper also presents a sentiment analysis of recent verified news to understand \\npeoples’ reaction psychologically, politically and economically. Table 2 shows the number of \\npositive and negative articles analysed from the news. \\nTable 2 Number of positive and negative articles from the news \\nSentiment  n  Sentiment  n ', 'Positive  561  Negative  2548 \\nFig. 11(a) and (b) show the word cloud for positive and negative sentiments, respectively. \\nIt was found that both sentiments have the word “China” but it is necessary to check the word \\nsequences to identify whether sentences that contain the word “China” are considered positive or \\nnegative. \\n \\n(a)  Positive sentiments \\n \\n(b) Negative sentiments \\nFig. 11 Word cloud for sentiments analysis on the news ', 'Top 5 positive and negative articles are represented in Table 3 and 4. \\nTable 3 Top 5 positive articles \\nNews  No. of words  Sentiment type  Score \\nMELBOURNE: The coach of the Chinese  24  POSITIVE  0.999845 \\nwomen\\'s soccer team has praised the \"strong \\nhearts\" of his players after they emerged \\nundefeated from an ... \\nWHO Director-General said solid collaboration,  13  POSITIVE  0.999836 \\ntransparency, prompt sharing of data, and \\naccurate ad \\nChinese President Xi Jinping has written a letter  23  POSITIVE  0.999809 \\nexpressing thanks to the Bill & Melinda Gates \\nFoundation for the organisation\\'s \"generosity\" \\nand ... \\nSINGAPORE: The reaction of Singaporeans, as  31  POSITIVE  0.999795 \\nthey pitch in to help with the Wuhan coronavirus \\nthreat, has been \\'amazing\\', said Law and Home \\nAffairs Minister K Shanmugam on Sunday (Feb \\n2). \\n\"We very much appreciate the efforts of the  35  POSITIVE  0.999698 \\ndoctors. They took good care of us,\" said a \\npatient surnamed Li while walking toward her ', \"family after bidding farewell to the medical \\nworkers in Hongshan Gymnasium. \\n \\nTable 4 Top 5 Negative Sentiments Articles \\nNews  No. of words  Sentiment type  Score \\nSINGAPORE: The Wuhan coronavirus will  31  NEGATIVE  0.966091 \\ncause current economic uncertainties to \\nescalate, but Singapore's economy is prepared \\nto weather the impending financial impact, said \\nManpower Minister Josephine Teo on \\nThursday (Jan 30). \\nBEIJING - A Chinese teenage boy suffering  39  NEGATIVE  0.936342 \\nfrom cerebral palsy was found dead at home in \\nHubei province six days after his father and \\nyounger brother were quarantined for \\nsuspected infection with a novel coronavirus.. \\nRead more at straitstimes.com. \\nAn image of a Chinese man collapsed in a  33  NEGATIVE  0.99643 \\nSeoul subway station, apparently too sick to \\nstand, has been held up by many Koreans as \\nproof of just how great a threat the \", 'It is not necessary to wear a face mask for now  26  NEGATIVE  0.771767 \\nsince no human-to-human transmission of the \\nnovel coronavirus (2019-nCoV) has occurred \\nin Malaysia, says infe... \\nUS researchers are already working on a  17  NEGATIVE  0.94461 \\nvaccine against the new virus that has emerged \\nin China. \\n \\n5. Discussion \\n5.1 Health information dissemination \\nInformation from news articles played an important role in empowering citizens during \\nan epidemic. However, our analysis found more negative articles than positive articles which is a \\nconcern. (2548 vs. 561) The word clouds corresponding to the negative and positive statements \\ndisplayed similar words. A content analysis of text mined Ebola on news articles and scientific \\npublications showed similar findings. According to An’s study, they found a difference in coverage \\nof topics but a word co-occurrence map shared similar entities, which showed the limitation of \\nsingle word tagging for sentiment analysis during an epidemic. An interface for citizens to record \\ntheir sentiments and expressing their opinions on the news articles could contribute towards \\nimproving these findings [30].  The top 5 positive articles were about collaboration and strength \\nof individuals in facing this epidemic, whereas the top 5 negative articles were related to \\nuncertainty and poor outcome of disease like death. Another study evaluating information shared \\nduring the Ebola epidemic also found news articles to place more focus on event-related entities \\nsuch as person, organization and location, while social media information like in Twitter places ', 'covers more time-oriented entities [31]. \\n5.2 Economic and politics impact \\nCOVID-19 is spreading with astounding speed and have severe consequences. The cases \\nincrease rapidly with major outbreaks in South Korea, Italy, Iran, the United States and more than \\n50 other countries. Governments are put to the test with escalating high-pressure and costs of the \\noutbreak in terms of public trust and the economy. Any mismanagement could carry political costs, \\nas their legitimacy and competency will be called into question by the people.  \\nTransparency is crucial and starts inside the government [32]. When the first outbreak \\nbegan, the public reacted to transparency, comprehensive and timely information and data provided. \\nWithholding the information created a vicious cycle of mistrust in authorities [33]. \\nClear direction in mitigating the outbreak is a must. The accurate and reliable information \\nare underpinned to ensure the containment effort is aggressively used and disseminated at any form \\nof social media to alert the public. In the absence of facts and trust, rumours and panic are inevitable \\nas people turned out to be emotional. These emotions created anger and fear that posed a threat to \\nthe government. According to Amesh Adalja, a senior scholar at the Johns Hopkins Center for \\nHealth Security, argued that the public is discouraged to view the response from the government \\nas a failure as that could create distrust of future public health measures [34].  \\nAt the geopolitical level, travel bans, border closing, trade controls, and others are other \\ncontainment measures to urgently enhance global readiness needed in response to Covid-19. The \\nconsequences of outbreaks and epidemics are not distributed equally and the economic impact is \\nalso highly uncertain. Some sectors may even benefit financially, while others will suffer \\ndisproportionately [35]. Most countries that are badly affected by Covid-19 have prepared the \\nstimulus package or domestic economic growth plan in order to boost spending activities.  ', 'According to Wei Yao, the chief Asia economist at Société Générale has stated that the \\nworse the economic data are right now, the more aggressive policy responses will be. Given the \\ncurrent development, the infrastructure stimulus is almost certain and the question is just how much \\n[36].  \\n6. Conclusion \\nThis research presented current trends of COVID-19 outbreak from 22nd Jan 2020 to 3rd \\nMarch 2020 as visualized in CoronaTracker website. The trajectory of the outbreak is also \\nforecasted by using SEIR model, 240 days from 20th January 2020. We also analyzed the sentiments \\nfrom news extracted by CoronaTracker to further understand people’s reaction towards this \\noutbreak. COVID-19 is still an infectious disease with some unclear or unknown properties, which \\nmeans accurate SEIR prediction can only be obtained once the outbreak has been successfully \\ncontained. The outbreak spreads are largely influenced by each country’s policy and social \\nresponsibility.  \\nIn a pandemic like this, providing timely information to the public is paramount. A \\nplatform like CoronaTracker will assist the government and authorities to disseminate verified \\narticles, provide updates to the situation, and advocate good personal hygiene to the people. \\nCoronaTracker is built out of social responsibility to spread awareness to the common people by \\nproviding scientific-based data analysis, prediction and verified news. Our website has attracted \\nmore than 1000 users viewing our page at one time and surpassed 4 million-page views on 16th \\nMarch 2020. Our platform is not limited to CoronaTracker webpage, but also Facebook page, \\nTwitter and LinkedIn, with our ultimate goal is to provide verified news and current statistics to \\nthe world.  \\nThis paper is still an ongoing research as many more investigations regarding this disease \\ncan be carried out. Yet, it serves as the starting phase to research more in depth on questions that ', 'revolve around this global pandemic. \\n \\nAcknowledgments \\nThis work is supported by Amazon AWS, LEAD and CirroLytix. We thank other \\nCoronaTracker core members, Tan Wei Seng, Poon Chee Him and Marcus Chia for managing the \\ncommunity and the website. We also thank Yiran Jing, Zi Qing Ang, Goh Kok Han, Nikky Ng, \\nDebby Huang, Shi Yong Pang, Roland Emmanuel Salunga and Kenny Kang for their involvement \\nin our team discussion. \\n \\nAuthor Group and Contributions \\nThe CoronaTracker Research Team includes Fairoza Amira Binti Hamzah, Cher Han Lau, \\nHafeez Nazri, Dominic Ligot, Ummi Hasanah Zaidon, Mohammad Khursani Bin Mohd Shaib, \\nGuanhua Lee, Tan Chen Liang, Adina Binti Abdullah, Chung Ming Hong, Chin Hwee Ong, and \\nChew Pei Ying. All team members jointly conceptualized the study, analyzed and interpreted that \\ndata, wrote and revised the manuscript and decided to submit for publication. \\nCorresponding  author:  Fairoza  Amira  Binti  Hamzah,  fairoza@kcg.ac.jp  or \\nfairozaamira@gmail.com . \\n \\nReferences \\n \\n[1]  D. Hui et al, \"The continuing 2019-nCoV epidemic threat of novel coronavirus to global \\nhealth - The latest 2019 novel coronavirus outbreak in Wuhan, China,\" International Journal \\nof Infectious Diseases, vol. 91, pp. 264-266, 2020.  \\n[2]  World Health Organization (WHO), \"Coronavirus disease 2019 (COVID-19) Situation ', 'Report - 35,\" WHO, 2020. \\n[3]  World Health Organization (WHO), \"Statement on the second meeting of the International \\nHealth  Regulations  (2005)  Emergency  Committee  regarding  the  outbreak  of  novel \\ncoronavirus (2019-nCov),\" WHO, 2020. \\n[4]  D. Toppenberg-Pejcic, J. Noyes, T. Allen, N. Alexander, M. Vanderford and G. Gamhewage, \\n\"Emergency Risk Communication: Lessons Learned from a Rapid Review of Recent Gray \\nLiterature on Ebola, Zika, and Yellow Fever,\" Health Communication, vol. 34, no. 4, pp. 437-\\n455, 2018.  \\n[5]  L. Lin, R. McCloud, C. Bigman and K. Viswanath, \"Tuning in and catching on? Examining \\nthe relationship between pandemic communication and awareness and knowledge of MERS \\nin the USA,\" Journal of Public Health, p. fdw028, 2016.  \\n[6]  WHO,  \"Advice  for  Public,\"  WHO  Int.,  2020.  [Online].  Available: \\nhttps://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public. \\n[Accessed 27 February 2020]. \\n[7]  World Health Organization, \"Report of the WHO-China Joint Mission on Coronavirus \\nDisease 2019 (COVID-19),\" World Health Organization, 2020. \\n[8]  \"Coronavirus Disease 2019 (COVID-19),\" Centers for Disease Control and Prevention, \\n2020.  [Online].  Available:  https://www.cdc.gov/coronavirus/2019-\\nncov/about/symptoms.html. [Accessed 27 February 2020]. \\n[9]  World Health Organization, \"2019 Novel Coronavirus (2019-nCoV): Strategic Preparedness \\nand Response Plan,\" World Health Organization, Geneva, 2020. \\n[10] World Health Organization, \"Coronavirus disease 2019 (COVID-19) Situation Report- 13,\" \\nWorld Health Organization, 2020. ', '[11] C. Chew and G. Eysenbach, \"Pandemics in the Age of Twitter: Content Analysis of Tweets \\nduring the 2009 H1N1 Outbreak,\" PLoS ONE, vol. 5, no. 11, p. e14118, 2010.  \\n[12] S. Oh, S. Lee and C. Han, \"The Effects of Social Media Use on Preventive Behaviors during \\nInfectious Disease Outbreaks: The Mediating Role of Self-relevant Emotions and Public \\nRisk Perception,\" Health Communication, pp. 1-10, 2020.  \\n[13] Kementerian Kesihatan Malaysia, \"COVID-19,\" Kementerian Kesihatan Malaysia, 2020. \\n[Online]. Available: https://www.facebook.com/kementeriankesihatanmalaysia/. [Accessed \\n27 February 2020]. \\n[14] Noor Hisham Abdullah, \"Noor Hisham Abdullah,\" Noor Hisham Abdullah, 2020. [Online]. \\nAvailable: https://www.facebook.com/DGHisham/. [Accessed 27 February 2020]. \\n[15] CoronaTracker Community , \"CoronaTracker,\" CoronaTracker, 2020. [Online]. Available: \\nhttps://www.coronatracker.com/. [Accessed 28 February 2020]. \\n[16] A. Rachah and D. F. M. Torres, \"Analysis, simulation and optimal control of a SEIR model \\nfor Ebola virus with demographic effects,\" Commun. Fac. Sac. Univ. Ank. Series A1, vol. 67, \\nno. 1, pp. 179-197, 2018.  \\n[17] A. T. Porter, \"A path-specific approach to SEIR modeling,\" Ph. D Thesis University of Iowa, \\n2012.  \\n[18] M. Munikar, S. Shakya and A. Shrestha, \"Fine-grained Sentiment Classification using \\nBERT,\" ArXiv, 2019.  \\n[19] X. Dong and G. de Melo, \"A Helping Hand: Transfer Learning for Deep Sentiment Analysis,\" \\nin Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, \\n2018.  \\n[20] U.  Upadhyay,  \"Knowledge  Distillation,\"  Medium,  2018.  [Online].  Available: ', 'https://medium.com/neuralmachine/knowledge-distillation-dc241d7c2322.  [Accessed  01 \\nMarch 2020]. \\n[21] John Hopkins University, \"Coronavirus Map,\" John Hopkins University, 17 March 2020. \\n[Online]. Available: https://coronavirus.jhu.edu/map.html. [Accessed 17 March 2020]. \\n[22] J. T. Wu, K. Leung, G. M. Leung, \"Nowcasting and forecasting the potential domestic and \\ninternational spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling \\nstudy,\" The Lancet, vol. 395, no. 10225, pp. 689-697, 2020.  \\n[23] The Institute for Disease Modelling, \"SEIR and SEIRs models,\" Institute for Disease \\nModelling,  2019.  [Online].  Available: \\nhttps://institutefordiseasemodeling.github.io/Documentation/general/model-seir.html. \\n[Accessed 03 March 2020]. \\n[24] Q. Li et al, \"Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus–Infected \\nPneumonia,\" The New England Journal of Medicine, pp. 1-9, 2020.  \\n[25] R. Socher et al, \"Recursive Deep Models for Semantic Compositionality Over a Sentiment \\nTreebank,\" in Conference on Empirical Methods in Natural Language Processing, 2013.  \\n[26] T. Wolf et al, \"HuggingFace\\'s Transformers: State-of-the-art Natural Language Processing,\" \\nArXiv, vol. abs/1910.03771, 2019.  \\n[27] CoronaTracker Analytics Team, \"Corona Tracker Analytics,\" CoronaTracker, 03 March \\n2020. [Online]. Available: https://www.coronatracker.com/analytics. [Accessed 03 March \\n2020]. \\n[28] V. Agafonkin, \"Leaflet,\" Leaflet, 2010. [Online]. Available: https://leafletjs.com/. [Accessed \\n7 March 2020]. \\n[29] SciPy  Developers,  \"SciPy,\"  SciPy,  2020.  [Online]. Available:  https://www.scipy.org/. ', '[Accessed 13 03 2020]. \\n[30] J. An, K. Ahn and M. Song, \"Text Mining Driven Content Analysis of Ebola on News Media \\nand Scientific Publications,\" Journal of the Korean Society for Library and Information \\nScience, vol. 50, no. 2, pp. 289-307, 2020.  \\n[31] Kim, Y. Jeong, Y. Kim, K. Kang and M. Song, \"Topic-based content and sentiment analysis \\nof Ebola virus on Twitter and in the news,\" Journal of Information Science, vol. 42, no. 6, \\npp. 763-781, 2016.  \\n[32] S. K. Khor, \"The Politics of the Coronavirus Outbreak,\" Think Global Health, 24 January \\n2020.  [Online]. Available:  https://www.thinkglobalhealth.org/article/politics-coronavirus-\\noutbreak. [Accessed 04 March 2020]. \\n[33] S. K. Khor, \"Malaysia does not have a good record of transparency,\" The Star, 15 January \\n2020.  [Online].  Available:  https://www.thestar.com.my/opinion/columnists/vital-\\nsigns/2020/01/15/malaysia-does-  not-have-a-good-record-of-transparency.  [Accessed  04 \\nMarch 2020]. \\n[34] J. Passy, \"Here\\'s why the U. S. government\\'s effort to contain the coronavirus outbreak was \\nnever going to be successful.,\" Market Watch, 29 February 2020. [Online]. Available: \\nhttps://www.marketwatch.com/story/heres-why-the-coronavirus-may-spread-in-the-united-\\nstates-despite-government-efforts-to-contain-the-outbreak-2020-02-27.  [Accessed  27 \\nFebruary 2020]. \\n[35] D. E. Bloom, D. Cadarette and JP Sevilla, \"New and resurgent infectious diseases can have \\nfar-reaching economic repercussions,\" Finance and Development, vol. 55, no. 2, pp. 46-49, \\n2018.  \\n[36] M. McCormick, \"Coronavirus: ECB ready to take \"targeted\" action to address economic ', 'impact of outbreak - as it happened,\" Financial Times, 2 March 2020. [Online]. Available: \\nhttps://www.ft.com/content/60f9e2ec-dd39-31cc-86d9-1adaa4dde1f8. [Accessed 04 March \\n2020]. \\n \\n APPENDIX: SEIR MODEL \\nIn these equations, 𝑆+𝐸+𝐼+𝑅=𝑁 is the total population, with rate of spread, β > \\n0, incubation rate σ > 0, and recovery rate γ > 0. The value of (cid:3031)(cid:3020) represents the rate of change 𝑆 \\n(cid:3031)(cid:3047)\\nwith respect to time 𝑡. Same as (cid:3031)(cid:3006), (cid:3031)(cid:3010), (cid:3031)(cid:3019). \\n(cid:3031)(cid:3047) (cid:3031)(cid:3047) (cid:3031)(cid:3047)\\nThe rate of spread, β is the rate of infection from an infected individual to one of their \\nsusceptible contacts on the unitary time step dt. For example, given two people A (infectious) and \\nB (susceptible), the probability of B becoming infected after contacting A during the unitary time \\nstep is β. The term ∆t is the difference between two observation points. Thus, number of \\nindividuals transferred from Susceptible state to Exposed state is \\n(cid:3081)(cid:3020)(cid:3010)𝛥𝑡,    (14) \\n(cid:3015)\\nwhere is the (cid:3081)(cid:3020)(cid:3010) is the Force of Infection in the SEIR model. Similarly, on the unitary time step, \\n(cid:3015)\\nthere are σE∆t number of cases transferred from Exposed state to Infectious, and γI (t)∆t number \\nof cases transferred from Infectious to Removed. \\nLet S(t), E(t), I(t) and R(t) be the number of susceptible, exposed, infectious and \\nremoved individuals at time t, then \\n𝑆(𝑡+∆𝑡)=𝑆(𝑡)−(cid:3081)(cid:3020)((cid:3047))(cid:3010)((cid:3047))∆𝑡,            (15) \\n(cid:3015)\\n𝐸(𝑡+∆𝑡)=𝐸(𝑡)+(cid:3081)(cid:3020)((cid:3047))(cid:3010)((cid:3047))∆(cid:3047)−𝜎𝐸(𝑡)∆𝑡,   (16) \\n(cid:3015)\\n𝐼(𝑡+∆𝑡)=𝐼(𝑡)+𝜎𝐸(𝑡)∆𝑡−𝛾𝐼(𝑡)∆𝑡,     (17) \\n𝑅(𝑡+∆𝑡)=𝑅(𝑡)+𝛾𝐼(𝑡)∆𝑡,              (18) ', 'Based on the definition of the first-order derivative,  (cid:3031)(cid:3025)=(cid:3025)((cid:3047)(cid:2878)∆(cid:3047))(cid:2879)(cid:3025)((cid:3047)) , as ∆𝑡 →0+ . Thus \\n(cid:3031)(cid:3047) ∆(cid:3047)\\nequation (15) – (18) can be rewritten as equation (10) – (13). \\nAssumptions of SEIR model [24] are as follows. \\ni.  The SEIR model assumes a closed population, which means that the total number of \\npopulations is fixed, no births, no death, or introduction new individuals. From equation \\n(10) – (13), we see that  (cid:3031) [𝑆(𝑡)+𝐸(𝑡)+𝐼(𝑡)+𝑅(𝑡)]=0, where the population N is \\n(cid:3031)(cid:3047)\\nconstant in any time 𝑡:𝑆(𝑡)+𝐸(𝑡)+𝐼(𝑡)+𝑅(𝑡)=𝑁 for any 𝑡 ≥0.  \\nii.  The individuals in the Exposed state are infected but not yet infectious.  \\niii.  Well-mixed population. \\niv.  SEIR model assumes that the latent and infectious times of the pathogen are exponentially \\ndistributed. \\nIn general, the dynamic SEIR is summarized as below [24]. \\na)  Start the epidemic with a group of Susceptible individuals and at least one Infectious \\nindividual.  \\nb)  The Infectious individuals mix with the Susceptible class and create Exposed individuals \\nfollowing a probabilistic process.  \\nc)  Exposed individuals spend some days without spreading the virus and based on another \\nprobabilistic process become additional Infectious class.  \\nd)  Newly Infectious class repeat #2 and create more Exposed class.  \\nInfectious individuals based on a probabilistic process either recover or die and become Removed \\nclass. ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ci2NaeA7Ebt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "passage = str(text_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVNk5-e26Gfm",
        "colab_type": "text"
      },
      "source": [
        "# Applying prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjgOXHQ4o1Ty",
        "colab_type": "text"
      },
      "source": [
        "## Using PyTorch simple QnA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDtgqKpWo87d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert import QA\n",
        "\n",
        "model = QA('model')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvWCHjCM31Ll",
        "colab_type": "code",
        "outputId": "1cbb91f9-30db-40a8-9fa5-c848be847e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result=model.predict(\n",
        "  passage=passage,\n",
        "  question=\"how it spreads?\" # Insert your question here\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(result[\"answer\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "through respiratory droplets and close contact\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoQWl-lQo_Sl",
        "colab_type": "text"
      },
      "source": [
        "## Using allennlp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MUKAcf_E3jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a21545e1-b798-4c69-b86e-a307ebdebbb0"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "predictor = Predictor.from_path(\"bidaf-model-2017.09.15-charpad.tar.gz\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1 [rnn.py:50]\n",
            "UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder. [token_characters_indexer.py:56]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goeF9JLY3x3C",
        "colab_type": "code",
        "outputId": "6684a9fd-33bc-490e-df65-3fe8e2341458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result=predictor.predict(\n",
        "  passage=passage,\n",
        "  question=\"how it spreads?\" # Insert your question here\n",
        ")\n",
        "result['best_span_str']"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'by each country’s policy and social \\\\nresponsibility'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2UKk4XsrLuv",
        "colab_type": "text"
      },
      "source": [
        "## These question and answers will be saved as source and target file for our prediction. In this context we used already available corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8-sIRzgMrUJ",
        "colab_type": "text"
      },
      "source": [
        "# SEQ2SEQ RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56C6AshoN4mF",
        "colab_type": "code",
        "outputId": "862c5be3-e16b-4ce3-9a38-f7424f30ee46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC51ivK3N9CT",
        "colab_type": "code",
        "outputId": "6cebd293-89c8-47fd-d839-427d6c81e1ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.VERSION"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0ABWP9kDXm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "892447e5-5390-44bd-cbb3-578e40de35c6"
      },
      "source": [
        "%cd \"sentiment_analysis\"\n",
        "!mkdir corpus"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seq2seq-RL-chatbot-with-PyTorch-QnA/sentiment_analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5HFR7tpti9M",
        "colab_type": "code",
        "outputId": "fb38d05e-3800-408f-c562-3f83b2cd2876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"corpus\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seq2seq-RL-chatbot-with-PyTorch-QnA/sentiment_analysis/corpus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvXSpFxBP57t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5e61240e-ab61-4c7b-dae6-68390641b95f"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1I1HLtix-R3O2HALyDwha6N4wM2K0p0FI&export=download"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I1HLtix-R3O2HALyDwha6N4wM2K0p0FI\n",
            "To: /content/seq2seq-RL-chatbot-with-PyTorch-QnA/sentiment_analysis/corpus/SAD.csv\n",
            "157MB [00:01, 86.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoANIG8NP-XG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "710250c1-03e3-462a-f858-8c3e5cbe6f30"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seq2seq-RL-chatbot-with-PyTorch-QnA/sentiment_analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7jr85D7DmOr",
        "colab_type": "text"
      },
      "source": [
        "# Training Sentimental Analysis model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fkCMRNtu18",
        "colab_type": "code",
        "outputId": "d35f4d9a-5d37-45d4-a1d1-c4d011c86c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files have already been formed!\n",
            "  Reading data line 100000\n",
            "  Reading data line 200000\n",
            "  Reading data line 300000\n",
            "  Reading data line 400000\n",
            "  Reading data line 500000\n",
            "  Reading data line 600000\n",
            "  Reading data line 700000\n",
            "  Reading data line 800000\n",
            "  Reading data line 900000\n",
            "  Reading data line 1000000\n",
            "  Reading data line 1100000\n",
            "  Reading data line 1200000\n",
            "  Reading data line 1300000\n",
            "  Reading data line 1400000\n",
            "  Reading data line 1500000\n",
            "WARNING:tensorflow:From run.py:141: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-06-07 05:56:12.627030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-06-07 05:56:12.642647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.643187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-06-07 05:56:12.643476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-07 05:56:12.645210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-07 05:56:12.646903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-07 05:56:12.647222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-07 05:56:12.649099: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-07 05:56:12.649997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-07 05:56:12.653585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-07 05:56:12.653691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.654236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.654786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-06-07 05:56:12.655138: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2020-06-07 05:56:12.660012: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000179999 Hz\n",
            "2020-06-07 05:56:12.660216: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a36a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-07 05:56:12.660238: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-06-07 05:56:12.743778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.744436: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a36bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-07 05:56:12.744466: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-06-07 05:56:12.744616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.745120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-06-07 05:56:12.745180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-07 05:56:12.745199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-07 05:56:12.745226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-07 05:56:12.745243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-07 05:56:12.745260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-07 05:56:12.745279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-07 05:56:12.745298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-07 05:56:12.745383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.745910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.746404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-06-07 05:56:12.746470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-07 05:56:12.747442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-07 05:56:12.747468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-06-07 05:56:12.747478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-06-07 05:56:12.747577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.748099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-07 05:56:12.748613: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-06-07 05:56:12.748658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From run.py:31: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From run.py:32: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From run.py:33: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From run.py:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From run.py:49: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From run.py:28: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Create model with fresh parameters\n",
            "WARNING:tensorflow:From run.py:123: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "2020-06-07 05:56:14.713624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "Train Loss: 0.17457464753091353\n",
            "Valid Loss: 0.15821995981037612\n",
            "Model Saved!\n",
            "Train Loss: 0.15169609455019226\n",
            "Valid Loss: 0.1553237557411194\n",
            "Model Saved!\n",
            "Train Loss: 0.14564108244702215\n",
            "Valid Loss: 0.14758071880787613\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Model Saved!\n",
            "Train Loss: 0.1429623256362977\n",
            "Valid Loss: 0.13279255963861944\n",
            "Model Saved!\n",
            "Train Loss: 0.1405404361002149\n",
            "Valid Loss: 0.13755188368260862\n",
            "Model Saved!\n",
            "Train Loss: 0.13660421412810683\n",
            "Valid Loss: 0.13353733360767364\n",
            "Model Saved!\n",
            "Train Loss: 0.13619034911319636\n",
            "Valid Loss: 0.13756451088935134\n",
            "Model Saved!\n",
            "Train Loss: 0.1352813109643755\n",
            "Valid Loss: 0.13773811019957066\n",
            "Model Saved!\n",
            "Train Loss: 0.13292809789255253\n",
            "Valid Loss: 0.13117365572601558\n",
            "Model Saved!\n",
            "Train Loss: 0.13066842511296267\n",
            "Valid Loss: 0.13469732657074932\n",
            "Model Saved!\n",
            "Train Loss: 0.13193160861544312\n",
            "Valid Loss: 0.1286713419109583\n",
            "Model Saved!\n",
            "Train Loss: 0.130409837152809\n",
            "Valid Loss: 0.14308684326708315\n",
            "Model Saved!\n",
            "Train Loss: 0.12985588531196118\n",
            "Valid Loss: 0.132789763212204\n",
            "Model Saved!\n",
            "Train Loss: 0.12739408731088048\n",
            "Valid Loss: 0.13812361415475607\n",
            "Model Saved!\n",
            "Train Loss: 0.1278924994450063\n",
            "Valid Loss: 0.1265645142644644\n",
            "Model Saved!\n",
            "Train Loss: 0.12806913241371515\n",
            "Valid Loss: 0.1274390159919858\n",
            "Model Saved!\n",
            "Train Loss: 0.12586685864627362\n",
            "Valid Loss: 0.13057580608874558\n",
            "Model Saved!\n",
            "Train Loss: 0.12656122934818245\n",
            "Valid Loss: 0.1324419237859547\n",
            "Model Saved!\n",
            "Train Loss: 0.12613529033400134\n",
            "Valid Loss: 0.12184813186526301\n",
            "Model Saved!\n",
            "Train Loss: 0.1261378554496913\n",
            "Valid Loss: 0.13029074467718602\n",
            "Model Saved!\n",
            "Train Loss: 0.1236440518870951\n",
            "Valid Loss: 0.13083427079021925\n",
            "Model Saved!\n",
            "Train Loss: 0.12553989797085524\n",
            "Valid Loss: 0.12564518310129644\n",
            "Model Saved!\n",
            "Train Loss: 0.12389209905266767\n",
            "Valid Loss: 0.13013906031846997\n",
            "Model Saved!\n",
            "Train Loss: 0.12350540500506764\n",
            "Valid Loss: 0.12746088575571782\n",
            "Model Saved!\n",
            "Train Loss: 0.1220810972601176\n",
            "Valid Loss: 0.13221052628010513\n",
            "Model Saved!\n",
            "Train Loss: 0.12151808690279725\n",
            "Valid Loss: 0.12788834746927025\n",
            "Model Saved!\n",
            "Train Loss: 0.12239174465462567\n",
            "Valid Loss: 0.12610798291862013\n",
            "Model Saved!\n",
            "Train Loss: 0.12285021479800343\n",
            "Valid Loss: 0.12986663503572346\n",
            "Model Saved!\n",
            "Train Loss: 0.12063569368235769\n",
            "Valid Loss: 0.13023772686719892\n",
            "Model Saved!\n",
            "Train Loss: 0.12218364129029217\n",
            "Valid Loss: 0.12759434979408982\n",
            "Model Saved!\n",
            "Train Loss: 0.1209902515783905\n",
            "Valid Loss: 0.12970465414226062\n",
            "Model Saved!\n",
            "Train Loss: 0.11990028431266557\n",
            "Valid Loss: 0.13024936564266684\n",
            "Model Saved!\n",
            "Train Loss: 0.11972007553838188\n",
            "Valid Loss: 0.12508747927844527\n",
            "Model Saved!\n",
            "Train Loss: 0.11962163593620054\n",
            "Valid Loss: 0.12968859314918518\n",
            "Model Saved!\n",
            "Train Loss: 0.11905929727852367\n",
            "Valid Loss: 0.1288531297072769\n",
            "Model Saved!\n",
            "Train Loss: 0.11989794419333334\n",
            "Valid Loss: 0.12383524879813194\n",
            "Model Saved!\n",
            "Train Loss: 0.11743754715844988\n",
            "Valid Loss: 0.13092822633683687\n",
            "Model Saved!\n",
            "Train Loss: 0.1176902234982698\n",
            "Valid Loss: 0.12905906980857254\n",
            "Model Saved!\n",
            "Train Loss: 0.11902651503495869\n",
            "Valid Loss: 0.1217718284577131\n",
            "Model Saved!\n",
            "Train Loss: 0.1168630067519846\n",
            "Valid Loss: 0.12434115108102561\n",
            "Model Saved!\n",
            "Train Loss: 0.11739468646980811\n",
            "Valid Loss: 0.1292855181545019\n",
            "Model Saved!\n",
            "Train Loss: 0.11487410933896894\n",
            "Valid Loss: 0.12658066604286428\n",
            "Model Saved!\n",
            "Train Loss: 0.1183008170519024\n",
            "Valid Loss: 0.1236593712866306\n",
            "Model Saved!\n",
            "Train Loss: 0.11533407954871654\n",
            "Valid Loss: 0.1288800483196974\n",
            "Model Saved!\n",
            "Train Loss: 0.11654092392046013\n",
            "Valid Loss: 0.12960139743983748\n",
            "Model Saved!\n",
            "Train Loss: 0.11781381134316321\n",
            "Valid Loss: 0.12328695658594374\n",
            "Model Saved!\n",
            "Train Loss: 0.11474253219738599\n",
            "Valid Loss: 0.12543427113443614\n",
            "Model Saved!\n",
            "Train Loss: 0.1155473937187344\n",
            "Valid Loss: 0.1292054309695959\n",
            "Model Saved!\n",
            "Train Loss: 0.11480004205554732\n",
            "Valid Loss: 0.12596187211573126\n",
            "Model Saved!\n",
            "Train Loss: 0.11514688786119207\n",
            "Valid Loss: 0.124149800427258\n",
            "Model Saved!\n",
            "Train Loss: 0.11546923681162298\n",
            "Valid Loss: 0.12233379181474444\n",
            "Model Saved!\n",
            "Train Loss: 0.11311917818337677\n",
            "Valid Loss: 0.12940454080700878\n",
            "Model Saved!\n",
            "Train Loss: 0.11333280919305974\n",
            "Valid Loss: 0.12165612850338223\n",
            "Model Saved!\n",
            "Train Loss: 0.11611229063384233\n",
            "Valid Loss: 0.12328971765935422\n",
            "Model Saved!\n",
            "Train Loss: 0.11389692745171483\n",
            "Valid Loss: 0.1227158649265766\n",
            "Model Saved!\n",
            "Train Loss: 0.11473451641667641\n",
            "Valid Loss: 0.13031542964279652\n",
            "Model Saved!\n",
            "Train Loss: 0.114994949489832\n",
            "Valid Loss: 0.12372063029557469\n",
            "Model Saved!\n",
            "Train Loss: 0.11503951102308925\n",
            "Valid Loss: 0.1297552943229676\n",
            "Model Saved!\n",
            "Train Loss: 0.11445481094345465\n",
            "Valid Loss: 0.12154377784579995\n",
            "Model Saved!\n",
            "Train Loss: 0.11497889844886944\n",
            "Valid Loss: 0.12757461436092854\n",
            "Model Saved!\n",
            "Train Loss: 0.11541654414497307\n",
            "Valid Loss: 0.13019770730286834\n",
            "Model Saved!\n",
            "Train Loss: 0.11141684372723092\n",
            "Valid Loss: 0.1263727729395032\n",
            "Model Saved!\n",
            "Train Loss: 0.11432715734653179\n",
            "Valid Loss: 0.12331610042601825\n",
            "Model Saved!\n",
            "Train Loss: 0.11124291468411678\n",
            "Valid Loss: 0.12921282418072227\n",
            "Model Saved!\n",
            "Train Loss: 0.11168144593015318\n",
            "Valid Loss: 0.12000573884695769\n",
            "Model Saved!\n",
            "Train Loss: 0.11219370602071285\n",
            "Valid Loss: 0.12079924125224349\n",
            "Model Saved!\n",
            "Train Loss: 0.11202974144741887\n",
            "Valid Loss: 0.12115222856402401\n",
            "Model Saved!\n",
            "Train Loss: 0.11056529894284897\n",
            "Valid Loss: 0.12623752903193236\n",
            "Model Saved!\n",
            "Train Loss: 0.11162191938422619\n",
            "Valid Loss: 0.12660406297072768\n",
            "Model Saved!\n",
            "Train Loss: 0.11275453631021078\n",
            "Valid Loss: 0.13089399188756945\n",
            "Model Saved!\n",
            "Train Loss: 0.11059270847216257\n",
            "Valid Loss: 0.1263215780258179\n",
            "Model Saved!\n",
            "Train Loss: 0.11128454272076495\n",
            "Valid Loss: 0.12533665914088488\n",
            "Model Saved!\n",
            "Train Loss: 0.11171167261339743\n",
            "Valid Loss: 0.12466217121109373\n",
            "Model Saved!\n",
            "Train Loss: 0.11218713101930924\n",
            "Valid Loss: 0.12494878582656384\n",
            "Model Saved!\n",
            "Train Loss: 0.11163919798471028\n",
            "Valid Loss: 0.12692717283964158\n",
            "Model Saved!\n",
            "Train Loss: 0.11251642218045885\n",
            "Valid Loss: 0.12747414737939838\n",
            "Model Saved!\n",
            "Train Loss: 0.11071404395252447\n",
            "Valid Loss: 0.1298188604414463\n",
            "Model Saved!\n",
            "Train Loss: 0.11041072549484675\n",
            "Valid Loss: 0.1282885402068496\n",
            "Model Saved!\n",
            "Train Loss: 0.10984559398703296\n",
            "Valid Loss: 0.12859084621071817\n",
            "Model Saved!\n",
            "Train Loss: 0.10946262943558381\n",
            "Valid Loss: 0.12319768950343128\n",
            "Model Saved!\n",
            "Train Loss: 0.1102475224304946\n",
            "Valid Loss: 0.12872526813298457\n",
            "Model Saved!\n",
            "Train Loss: 0.10868745600245897\n",
            "Valid Loss: 0.1289417161047458\n",
            "Model Saved!\n",
            "Train Loss: 0.11010574832186111\n",
            "Valid Loss: 0.1335117813944817\n",
            "Model Saved!\n",
            "Train Loss: 0.1091136893909424\n",
            "Valid Loss: 0.11846978314220909\n",
            "Model Saved!\n",
            "Train Loss: 0.11036451845057291\n",
            "Valid Loss: 0.12411300560459496\n",
            "Model Saved!\n",
            "Train Loss: 0.10853816468827424\n",
            "Valid Loss: 0.12882797002792362\n",
            "Model Saved!\n",
            "Train Loss: 0.11174003488570446\n",
            "Valid Loss: 0.12481084492057562\n",
            "Model Saved!\n",
            "Train Loss: 0.11121184542775159\n",
            "Valid Loss: 0.12275339283049104\n",
            "Model Saved!\n",
            "Train Loss: 0.10865293149463834\n",
            "Valid Loss: 0.12046020649373526\n",
            "Model Saved!\n",
            "Train Loss: 0.10828005578182633\n",
            "Valid Loss: 0.12775113079696893\n",
            "Model Saved!\n",
            "Train Loss: 0.10845326434448382\n",
            "Valid Loss: 0.1228922825306654\n",
            "Model Saved!\n",
            "Train Loss: 0.10915391956083494\n",
            "Valid Loss: 0.12522601496428248\n",
            "Model Saved!\n",
            "Train Loss: 0.10907054715789846\n",
            "Valid Loss: 0.12447787776589396\n",
            "Model Saved!\n",
            "Train Loss: 0.10737754138000298\n",
            "Valid Loss: 0.12586191225796936\n",
            "Model Saved!\n",
            "Train Loss: 0.10822608334943648\n",
            "Valid Loss: 0.11689816676080225\n",
            "Model Saved!\n",
            "Train Loss: 0.109979998392053\n",
            "Valid Loss: 0.12458656733855607\n",
            "Model Saved!\n",
            "Train Loss: 0.1104805497918278\n",
            "Valid Loss: 0.12789973203092814\n",
            "Model Saved!\n",
            "Train Loss: 0.10819294723495847\n",
            "Valid Loss: 0.1194987266883254\n",
            "Model Saved!\n",
            "Train Loss: 0.10747915684804317\n",
            "Valid Loss: 0.12788176927715544\n",
            "Model Saved!\n",
            "Train Loss: 0.10766085387952622\n",
            "Valid Loss: 0.12746871210634708\n",
            "Model Saved!\n",
            "Train Loss: 0.10828022391442217\n",
            "Valid Loss: 0.13469610761851067\n",
            "Model Saved!\n",
            "Train Loss: 0.10884326328616582\n",
            "Valid Loss: 0.12338167276233436\n",
            "Model Saved!\n",
            "Train Loss: 0.10688622215949\n",
            "Valid Loss: 0.1185160821676254\n",
            "Model Saved!\n",
            "Train Loss: 0.10744062152504898\n",
            "Valid Loss: 0.12811351224780085\n",
            "Model Saved!\n",
            "Train Loss: 0.10692027387209246\n",
            "Valid Loss: 0.12128765359520911\n",
            "Model Saved!\n",
            "Train Loss: 0.10695599323511122\n",
            "Valid Loss: 0.12344136252999298\n",
            "Model Saved!\n",
            "Train Loss: 0.1089412196408958\n",
            "Valid Loss: 0.1234923481941223\n",
            "Model Saved!\n",
            "Train Loss: 0.1091440523117781\n",
            "Valid Loss: 0.1301772036962212\n",
            "Model Saved!\n",
            "Train Loss: 0.10757870670966806\n",
            "Valid Loss: 0.12894971109926692\n",
            "Model Saved!\n",
            "Train Loss: 0.10727671364322312\n",
            "Valid Loss: 0.12212739955633878\n",
            "Model Saved!\n",
            "Train Loss: 0.10853142050094904\n",
            "Valid Loss: 0.1268823485448957\n",
            "Model Saved!\n",
            "Train Loss: 0.10696817818284027\n",
            "Valid Loss: 0.12788097027689216\n",
            "Model Saved!\n",
            "Train Loss: 0.10840878549404445\n",
            "Valid Loss: 0.12267703462392096\n",
            "Model Saved!\n",
            "Train Loss: 0.10814743357524298\n",
            "Valid Loss: 0.12199478730559345\n",
            "Model Saved!\n",
            "Train Loss: 0.10686907309107482\n",
            "Valid Loss: 0.11654259555041786\n",
            "Model Saved!\n",
            "Train Loss: 0.10700943343900131\n",
            "Valid Loss: 0.12630146618932486\n",
            "Model Saved!\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 193, in <module>\n",
            "    train()\n",
            "  File \"run.py\", line 157, in train\n",
            "    loss_train = Model.step(sess, encoder_input, encoder_length, target)\n",
            "  File \"run.py\", line 67, in step\n",
            "    outputs = session.run(output_feed, input_feed)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BEeGdXlqW2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLcYyxUsDuss",
        "colab_type": "text"
      },
      "source": [
        "# Training Seq2Seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuH0WfAzZ60Y",
        "colab_type": "code",
        "outputId": "47d20781-454b-452d-d899-ec217397f375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run.py --mode MLE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Map file has already been formed!\n",
            "Token file corpus/source.token has already existed!\n",
            "Token file corpus/target.token has already existed!\n",
            "  reading data line 100000\n",
            "  reading data line 200000\n",
            "Total document size: 149689\n",
            "2020-06-06 20:54:41.443658: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-06-06 20:54:41.561110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-06 20:54:41.561647: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2206100 executing computations on platform CUDA. Devices:\n",
            "2020-06-06 20:54:41.561681: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n",
            "2020-06-06 20:54:41.563247: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000160000 Hz\n",
            "2020-06-06 20:54:41.563505: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2206260 executing computations on platform Host. Devices:\n",
            "2020-06-06 20:54:41.563542: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2020-06-06 20:54:41.563685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 7.43GiB freeMemory: 7.32GiB\n",
            "2020-06-06 20:54:41.563710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2020-06-06 20:54:41.564262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-06 20:54:41.564286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2020-06-06 20:54:41.564296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2020-06-06 20:54:41.564352: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-06-06 20:54:41.564388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7123 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Seq2seq-Chatbot-With-Deep-Reinforcement-Learning/seq2seq_model.py:125: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Seq2seq-Chatbot-With-Deep-Reinforcement-Learning/seq2seq_model.py:128: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Seq2seq-Chatbot-With-Deep-Reinforcement-Learning/seq2seq.py:605: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Reading model from model/MLE.ckpt-900, mode: MLE\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2020-06-06 20:55:48.681579: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "Step 300, Training perplexity: 49.120700587216724, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 34.867732538313746\n",
            "  Validation perplexity in bucket 1: 35.79720766285506\n",
            "  Validation perplexity in bucket 2: 49.308950748696006\n",
            "  Validation perplexity in bucket 3: 75.85412760765414\n",
            "Saving model at step 300\n",
            "Step 600, Training perplexity: 43.60091076181677, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 21.33745851209865\n",
            "  Validation perplexity in bucket 1: 27.140175982284404\n",
            "  Validation perplexity in bucket 2: 39.79043877968416\n",
            "  Validation perplexity in bucket 3: 42.374175822958186\n",
            "Saving model at step 600\n",
            "Step 900, Training perplexity: 41.32941605600405, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 22.136473211550385\n",
            "  Validation perplexity in bucket 1: 29.189014125298883\n",
            "  Validation perplexity in bucket 2: 35.34031484630983\n",
            "  Validation perplexity in bucket 3: 43.95617485006303\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Saving model at step 900\n",
            "Step 1200, Training perplexity: 37.79672706738178, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 26.398956677865087\n",
            "  Validation perplexity in bucket 1: 29.330796206188303\n",
            "  Validation perplexity in bucket 2: 32.877187114894724\n",
            "  Validation perplexity in bucket 3: 55.90831472126214\n",
            "Saving model at step 1200\n",
            "Step 1500, Training perplexity: 36.55300067764719, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 17.449402053301775\n",
            "  Validation perplexity in bucket 1: 31.166139928079936\n",
            "  Validation perplexity in bucket 2: 36.5435268472017\n",
            "  Validation perplexity in bucket 3: 37.97924779235271\n",
            "Saving model at step 1500\n",
            "Step 1800, Training perplexity: 33.345459693142026, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 22.480826541408305\n",
            "  Validation perplexity in bucket 1: 18.973377163850795\n",
            "  Validation perplexity in bucket 2: 31.084043663501635\n",
            "  Validation perplexity in bucket 3: 36.081805185726616\n",
            "Saving model at step 1800\n",
            "Step 2100, Training perplexity: 32.988233808095416, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 19.986181944052074\n",
            "  Validation perplexity in bucket 1: 23.658546466632842\n",
            "  Validation perplexity in bucket 2: 29.782561694909734\n",
            "  Validation perplexity in bucket 3: 32.62214368832395\n",
            "Saving model at step 2100\n",
            "Step 2400, Training perplexity: 31.467656804050492, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 18.550623083835543\n",
            "  Validation perplexity in bucket 1: 22.98715034204685\n",
            "  Validation perplexity in bucket 2: 28.174936012479225\n",
            "  Validation perplexity in bucket 3: 35.0081236610997\n",
            "Saving model at step 2400\n",
            "Step 2700, Training perplexity: 30.83314482562207, Learning rate: 0.5\n",
            "  Validation perplexity in bucket 0: 16.25350569284928\n",
            "  Validation perplexity in bucket 1: 26.82584446203454\n",
            "  Validation perplexity in bucket 2: 27.27055613988245\n",
            "  Validation perplexity in bucket 3: 36.09083901780196\n",
            "Saving model at step 2700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPD2nXNiD0eD",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmYvf0VppMN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run.py --mode RL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttXJ-Q_jD9MZ",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BtKVFnND_nD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run.py --mode TEST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_dxPZNtysWI",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analyser\n",
        "# With an accuracy of 96%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYEW-_Npyyn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "# loading\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0lxEsFmyzXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import model_from_json\n",
        "# load json and create model\n",
        "json_file = open('model_conv_lstm_final.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model_conv_lstm_final.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPJz2g9cy4O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "twt = [\"Hello how are you?\"]# put your text here\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt = tokenizer.texts_to_sequences(twt)\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "twt = pad_sequences(twt, maxlen=100, dtype='int32', value=0)\n",
        "print(twt)\n",
        "sentiment = loaded_model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "print(np.argmax(sentiment))\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}